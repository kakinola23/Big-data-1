{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans, KMeansModel\n",
    "from pyspark.mllib.clustering import StreamingKMeans\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from numpy import array\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import scipy.sparse\n",
    "from pyspark.ml.linalg import Vectors, _convert_to_vector, VectorUDT\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.ml.linalg import SparseVector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('parquet').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|         value|\n",
      "+--------------+\n",
      "|P,P,A,A,A,P,NB|\n",
      "|N,N,A,A,A,N,NB|\n",
      "|A,A,A,A,A,A,NB|\n",
      "|P,P,P,P,P,P,NB|\n",
      "|N,N,P,P,P,N,NB|\n",
      "|A,A,P,P,P,A,NB|\n",
      "|P,P,A,P,P,P,NB|\n",
      "|P,P,P,A,A,P,NB|\n",
      "|P,P,A,P,A,P,NB|\n",
      "|P,P,A,A,P,P,NB|\n",
      "|P,P,P,P,A,P,NB|\n",
      "|P,P,P,A,P,P,NB|\n",
      "|N,N,A,P,P,N,NB|\n",
      "|N,N,P,A,A,N,NB|\n",
      "|N,N,A,P,A,N,NB|\n",
      "|N,N,A,P,A,N,NB|\n",
      "|N,N,A,A,P,N,NB|\n",
      "|N,N,P,P,A,N,NB|\n",
      "|N,N,P,A,P,N,NB|\n",
      "|A,A,A,P,P,A,NB|\n",
      "+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"text\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"Qualitative_Bankruptcy.txt\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label_point=1.0, features=SparseVector(5, {0: 3.0, 1: 3.0, 2: 2.0, 3: 2.0, 4: 2.0})),\n",
       " Row(label_point=1.0, features=SparseVector(5, {0: 1.0, 1: 1.0, 2: 2.0, 3: 2.0, 4: 2.0})),\n",
       " Row(label_point=1.0, features=SparseVector(5, {0: 2.0, 1: 2.0, 2: 2.0, 3: 2.0, 4: 2.0}))]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dense_to_sparse(vector):\n",
    "    return _convert_to_vector(scipy.sparse.csc_matrix(array(vector)).T)\n",
    "\n",
    "def getDoubleValue(d):\n",
    "    if d == 'A':\n",
    "        return 2.0\n",
    "    elif d == 'P':\n",
    "        return 3.0\n",
    "    elif d == 'N':\n",
    "        return 1.0\n",
    "    elif d == 'B':\n",
    "        return 0.0\n",
    "    elif d == 'NB':\n",
    "        return 1.0\n",
    "    \n",
    "    \n",
    "def doTheThing(parts):\n",
    "    label_point = getDoubleValue(parts[6])\n",
    "    parts[0:6] = map(getDoubleValue ,parts[0:6])\n",
    "\n",
    "    parts = parts[0:6]\n",
    "    parts.insert(0, label_point)\n",
    "    \n",
    "    parts = Row(label_point = parts[0], features = dense_to_sparse(parts[1:6]))\n",
    "    return parts\n",
    "\n",
    "df2 = df.rdd.map(lambda d: doTheThing(d[0].split(',')))\n",
    "\n",
    "df2 = df2.toDF()\n",
    "\n",
    "df2.take(3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans().setK(2).setSeed(10)\n",
    "model = km.fit(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1.75454545, 1.4       , 1.06363636, 1.21818182, 1.08181818]),\n",
       " array([2.12857143, 2.06428571, 2.29285714, 2.50714286, 2.63571429])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cts = model.clusterCenters()\n",
    "cts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+----------+\n",
      "|label_point|            features|prediction|\n",
      "+-----------+--------------------+----------+\n",
      "|        1.0|(5,[0,1,2,3,4],[3...|         1|\n",
      "|        1.0|(5,[0,1,2,3,4],[1...|         0|\n",
      "|        1.0|(5,[0,1,2,3,4],[2...|         1|\n",
      "|        1.0|(5,[0,1,2,3,4],[3...|         1|\n",
      "|        1.0|(5,[0,1,2,3,4],[1...|         1|\n",
      "|        1.0|(5,[0,1,2,3,4],[2...|         1|\n",
      "|        1.0|(5,[0,1,2,3,4],[3...|         1|\n",
      "|        1.0|(5,[0,1,2,3,4],[3...|         1|\n",
      "|        1.0|(5,[0,1,2,3,4],[3...|         1|\n",
      "|        1.0|(5,[0,1,2,3,4],[3...|         1|\n",
      "|        1.0|(5,[0,1,2,3,4],[3...|         1|\n",
      "|        1.0|(5,[0,1,2,3,4],[3...|         1|\n",
      "|        1.0|(5,[0,1,2,3,4],[1...|         1|\n",
      "|        1.0|(5,[0,1,2,3,4],[1...|         1|\n",
      "|        1.0|(5,[0,1,2,3,4],[1...|         1|\n",
      "|        1.0|(5,[0,1,2,3,4],[1...|         1|\n",
      "|        1.0|(5,[0,1,2,3,4],[1...|         1|\n",
      "|        1.0|(5,[0,1,2,3,4],[1...|         1|\n",
      "|        1.0|(5,[0,1,2,3,4],[1...|         1|\n",
      "|        1.0|(5,[0,1,2,3,4],[2...|         1|\n",
      "+-----------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trans = model.transform(df2)\n",
    "trans.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|accuracy|\n",
      "+--------+\n",
      "|    0.98|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def correctPredict(label_point, prediction):\n",
    "    if label_point == prediction:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "correctPredict = udf(correctPredict, IntegerType())\n",
    "\n",
    "trans = trans.withColumn('correctPredict', correctPredict('label_point', 'prediction'))\n",
    "#Accuracy  of the implementation\n",
    "trans.agg((sum('correctPredict')/count('label_point')).alias(\"accuracy\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: \n",
      "[1.75454545 1.4        1.06363636 1.21818182 1.08181818]\n",
      "[2.12857143 2.06428571 2.29285714 2.50714286 2.63571429]\n"
     ]
    }
   ],
   "source": [
    "# results\n",
    "cluster_centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in cluster_centers:\n",
    "    print(center)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
